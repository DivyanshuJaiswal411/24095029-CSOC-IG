{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d608b4f8",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ceb99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence,to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a1d8c",
   "metadata": {},
   "source": [
    "### importing data and spliting into test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8139e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\Divyanshu\\Downloads\\archive\\train.csv\", header=None, names=[\"label\", \"text\"],quotechar='\"',encoding='utf-8')\n",
    "test=pd.read_csv(r\"C:\\Users\\Divyanshu\\Downloads\\archive\\test.csv\", header=None, names=[\"label\", \"text\"],quotechar='\"',encoding='utf-8')\n",
    "train['label'] = train['label'].map({1: 0, 2: 1})\n",
    "test['label'] = test['label'].map({1: 0, 2: 1})\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train['text'], train['label'], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e0eca",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8820d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert POS tags to WordNet format\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess(text):\n",
    "    #remove HTML tags\n",
    "    text=re.sub('<[^>]*>','',text)\n",
    "    #remove non-alphanumeric characters and convert into lowercase\n",
    "    text=re.sub('[^a-zA-Z]',' ',text).lower()\n",
    "    #tokenization\n",
    "    words=word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[word for word in words if word not in stop_words]\n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    \n",
    "    preprocessed_text=' '.join(words)\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056dd415",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e1df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 200\n",
    "embedding_dim = 300\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249bfd96",
   "metadata": {},
   "source": [
    "###  Preprocessing and tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ccd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing train texts...\")\n",
    "train_texts_clean = [preprocess(text) for text in train_texts]\n",
    "print(\"Preprocessing validation texts...\")\n",
    "val_texts_clean = [preprocess(text) for text in val_texts]\n",
    "print(\"Preprocessing test texts...\")\n",
    "test_texts_clean = [preprocess(text) for text in test['text']]\n",
    "\n",
    "# --- Prepare labels as integers (0 or 1) ---\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test['label'])\n",
    "\n",
    "# --- Tokenizer ---\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f80fea8",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d4d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec model...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    if word in w2v_model:\n",
    "        embedding_matrix[i] = w2v_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18522ba1",
   "metadata": {},
   "source": [
    "### Preprocessing and converting into one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e9b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(texts, labels, tokenizer, max_len):\n",
    "    for text, label in zip(texts, labels):\n",
    "        seq = tokenizer.texts_to_sequences([text])\n",
    "        pad_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        yield pad_seq[0], label\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(max_len,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_texts_clean, y_train, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_texts_clean, y_val, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_texts_clean, y_test, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155ae37",
   "metadata": {},
   "source": [
    "### RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90883592",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=False),\n",
    "    SimpleRNN(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb94f7",
   "metadata": {},
   "source": [
    "### Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d39201",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#Gather predictions and true labels \n",
    "true_labels = []\n",
    "pred_probs = []\n",
    "\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    preds = model.predict(x_batch).flatten()\n",
    "    pred_probs.extend(preds)\n",
    "    true_labels.extend(y_batch.numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_probs = np.array(pred_probs)\n",
    "pred_labels = (pred_probs >= 0.5).astype(int)\n",
    "\n",
    "#Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Classification Report\n",
    "report = classification_report(true_labels, pred_labels, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "#PR-AUC Curve\n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
