{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f6d0a6",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4d439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Divyanshu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc\n",
    "\n",
    "tqdm.pandas()\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358642e",
   "metadata": {},
   "source": [
    "### importing data and splitting into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af156723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\Divyanshu\\Downloads\\archive\\train.csv\", header=None, names=[\"label\", \"text\"],quotechar='\"',encoding='utf-8')\n",
    "test=pd.read_csv(r\"C:\\Users\\Divyanshu\\Downloads\\archive\\test.csv\", header=None, names=[\"label\", \"text\"],quotechar='\"',encoding='utf-8')\n",
    "train['label'] = train['label'].map({1: 0, 2: 1})\n",
    "test['label'] = test['label'].map({1: 0, 2: 1})\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train['text'], train['label'], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b827643",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10baffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert POS tags to WordNet format\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {'J': wordnet.ADJ, 'N': wordnet.NOUN, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess(text):\n",
    "    #remove HTML tags\n",
    "    text=re.sub('<[^>]*>','',text)\n",
    "    #remove non-alphanumeric characters and convert into lowercase\n",
    "    text=re.sub('[^a-zA-Z]',' ',text).lower()\n",
    "    #tokenization\n",
    "    words=word_tokenize(text)\n",
    "    #Remove stopwords\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[word for word in words if word not in stop_words]\n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    \n",
    "    preprocessed_text=' '.join(words)\n",
    "    return preprocessed_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4038e08c",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2876d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "max_len = 200\n",
    "embedding_dim = 300\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a19d706",
   "metadata": {},
   "source": [
    "### Preprocessing and Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ecad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preprocessing train texts...\")\n",
    "train_texts_clean = [preprocess(text) for text in train_texts]\n",
    "print(\"Preprocessing validation texts...\")\n",
    "val_texts_clean = [preprocess(text) for text in val_texts]\n",
    "print(\"Preprocessing test texts...\")\n",
    "test_texts_clean = [preprocess(text) for text in test['text']]\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n",
    "y_test = np.array(test['label'])\n",
    "\n",
    "#Tokenizer \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_texts_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514a1731",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2636c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Word2Vec model...\")\n",
    "w2v_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(max_words, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_words:\n",
    "        continue\n",
    "    if word in w2v_model:\n",
    "        embedding_matrix[i] = w2v_model[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8669ad52",
   "metadata": {},
   "source": [
    "### Preprocessing and converting into one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e004b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(texts, labels, tokenizer, max_len):\n",
    "    for text, label in zip(texts, labels):\n",
    "        seq = tokenizer.texts_to_sequences([text])\n",
    "        pad_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        yield pad_seq[0], label\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(max_len,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(train_texts_clean, y_train, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").shuffle(10000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(val_texts_clean, y_val, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(test_texts_clean, y_test, tokenizer, max_len),\n",
    "    output_signature=output_signature\n",
    ").batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3a9496",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432157d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=num_words,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,\n",
    "              trainable=False),\n",
    "    LSTM(128, activation='tanh'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d6bd5",
   "metadata": {},
   "source": [
    "### Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01740af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate on test set \n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {loss:.4f} | Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "#Gather predictions and true labels\n",
    "true_labels = []\n",
    "pred_probs = []\n",
    "\n",
    "for x_batch, y_batch in test_dataset:\n",
    "    preds = model.predict(x_batch).flatten()\n",
    "    pred_probs.extend(preds)\n",
    "    true_labels.extend(y_batch.numpy())\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "pred_probs = np.array(pred_probs)\n",
    "pred_labels = (pred_probs >= 0.5).astype(int)\n",
    "\n",
    "#Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "#Classification Report \n",
    "report = classification_report(true_labels, pred_labels, target_names=['Negative', 'Positive'])\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "#PR-AUC Curve \n",
    "precision, recall, thresholds = precision_recall_curve(true_labels, pred_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall, precision, label=f'PR-AUC = {pr_auc:.4f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
